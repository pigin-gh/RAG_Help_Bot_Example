"""
RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π.
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.
"""
import sys
import os
import re
from pathlib import Path
from typing import List, Optional, Tuple

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    # Fallback –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from tqdm import tqdm


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ RAG"""
    
    def __init__(self, knowledge_base_path: str, vectorstore_dir: Optional[str] = None,
                 chunk_size: int = 1000, chunk_overlap: int = 200,
                 use_hybrid_search: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG —Å–µ—Ä–≤–∏—Å
        
        Args:
            knowledge_base_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
            vectorstore_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: vectorstore/)
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            chunk_overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            use_hybrid_search: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25)
        """
        self.knowledge_base_path = Path(knowledge_base_path)
        self.vectorstore_dir = Path(vectorstore_dir) if vectorstore_dir else Path(__file__).parent / "vectorstore"
        self.vectorstore_dir.mkdir(exist_ok=True)
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.use_hybrid_search = use_hybrid_search
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.vectorstore = None
        self.embeddings = None
        self.bm25_index = None
        self.chunks_for_bm25 = None
        self.documents = None  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è BM25
        
        self._initialize()
    
    def _get_index_path(self) -> Path:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
        kb_name = self.knowledge_base_path.stem
        return self.vectorstore_dir / f"{kb_name}_faiss_index"
    
    def _should_rebuild_index(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å, False –µ—Å–ª–∏ –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
        """
        index_path = self._get_index_path()
        
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
        if not index_path.exists():
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        kb_mtime = os.path.getmtime(self.knowledge_base_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞
        # –ò—â–µ–º —Ñ–∞–π–ª index.faiss (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞)
        index_file = index_path / "index.faiss"
        if not index_file.exists():
            return True
        
        index_mtime = os.path.getmtime(index_file)
        
        # –ï—Å–ª–∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–æ–≤–µ–µ –∏–Ω–¥–µ–∫—Å–∞, –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å
        return kb_mtime > index_mtime
    
    def _build_bm25_index(self):
        """–°—Ç—Ä–æ–∏—Ç BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞"""
        try:
            from rank_bm25 import BM25Okapi
        except ImportError:
            print("   ‚ö† rank-bm25 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install rank-bm25")
            print("   –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫.")
            self.bm25_index = None
            return

        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è BM25
        self.chunks_for_bm25 = [doc.page_content.split() for doc in self.documents]
        self.bm25_index = BM25Okapi(self.chunks_for_bm25)
    
    def _initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ —Å–æ–∑–¥–∞–µ—Ç/–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        print("\n" + "="*60)
        print("üìö –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø RAG –°–ï–†–í–ò–°–ê")
        print("="*60)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings (–Ω—É–∂–Ω–∞ –≤—Å–µ–≥–¥–∞)
        print("\n[1/4] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ embeddings...")
        print("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 –º–∏–Ω—É—Ç—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
        print("   –ú–æ–¥–µ–ª—å: intfloat/multilingual-e5-base")
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace...")
        sys.stdout.flush()
        
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-base",
                model_kwargs={'device': 'cpu'}
            )
            print("   ‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        except Exception as e:
            print(f"   ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
        
        index_path = self._get_index_path()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        if not self._should_rebuild_index():
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            print(f"\n[2/4] –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            print(f"   –ü—É—Ç—å: {index_path}")
            try:
                self.vectorstore = FAISS.load_local(
                    str(index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("   ‚úì –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                
                # –î–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å BM25 –∏–Ω–¥–µ–∫—Å
                # –ù–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞, –ø–æ—ç—Ç–æ–º—É
                # –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –±—É–¥–µ—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                if self.use_hybrid_search:
                    print("   ‚ö† –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞.")
                    print("   –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–π—Ç–µ –∏–Ω–¥–µ–∫—Å.")
                
                print("\n" + "="*60)
                print("‚úÖ –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
                print("="*60 + "\n")
                return
            except Exception as e:
                print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                print("   –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        print("\n[2/4] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞...")
        loader = TextLoader(
            str(self.knowledge_base_path),
            encoding='utf-8'
        )
        documents = loader.load()
        print(f"   ‚úì –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç(–æ–≤)")
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        print("\n[3/4] –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=["\n\n\n", "\n\n", "\n", ". ", " ", ""],
            keep_separator=True,
        )
        chunks = text_splitter.split_documents(documents)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫ –∫–∞–∂–¥–æ–º—É —á–∞–Ω–∫—É
        for chunk in chunks:
            if 'source' not in chunk.metadata:
                chunk.metadata['source'] = str(self.knowledge_base_path.name)
            chunk.metadata['document_type'] = 'knowledge_base'
        
        self.documents = chunks
        print(f"   ‚úì –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        print(f"\n[4/4] –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        print("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 –º–∏–Ω—É—Ç—ã, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...)")
        print("   –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –∏–Ω–¥–µ–∫—Å–∞ FAISS...")
        sys.stdout.flush()
        
        try:
            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
            print("   ‚úì –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫
            print(f"\n[5/5] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫...")
            print(f"   –ü—É—Ç—å: {index_path}")
            self.vectorstore.save_local(str(index_path))
            print("   ‚úì –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
            
            # –°—Ç—Ä–æ–∏–º BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
            if self.use_hybrid_search:
                print("\n[6/6] –°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
                self._build_bm25_index()
                if self.bm25_index:
                    print("   ‚úì BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤")
                else:
                    print("   ‚ö† BM25 –∏–Ω–¥–µ–∫—Å –Ω–µ —Å–æ–∑–¥–∞–Ω (–±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞)")
        except Exception as e:
            print(f"\n   ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
        
        print("\n" + "="*60)
        print("‚úÖ –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
        print("="*60 + "\n")
    
    def _advanced_rerank_results(self, query: str, results: List[Tuple[Document, float]],
                               top_k: int = None, user_context: dict = None) -> List[Tuple[Document, float]]:
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π re-ranking —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º —Ñ–∞–∫—Ç–æ—Ä–æ–≤"""
        if top_k is None:
            top_k = len(results)

        query_lower = query.lower()
        query_words = set(re.findall(r'\b\w+\b', query_lower))

        reranked = []
        for doc, base_score in results:
            factors = self._calculate_advanced_rerank_factors(query, query_words, doc, user_context)

            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ñ–∞–∫—Ç–æ—Ä–æ–≤
            final_score = (
                base_score * 0.5 +           # –ë–∞–∑–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π/BM25 score
                factors['word_overlap'] * 0.2 +     # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤
                factors['semantic_match'] * 0.15 +  # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                factors['authority'] * 0.1 +         # –ê–≤—Ç–æ—Ä–∏—Ç–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞
                factors['recency'] * 0.05             # –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å
            )

            reranked.append((doc, final_score))

        reranked.sort(key=lambda x: x[1], reverse=True)
        return reranked[:top_k]

    def _calculate_advanced_rerank_factors(self, query: str, query_words: set,
                                         doc: Document, user_context: dict = None) -> dict:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ re-ranking"""
        content = doc.page_content.lower()
        metadata = doc.metadata

        factors = {}

        # 1. Word overlap (—Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å–ª–æ–≤)
        content_words = set(re.findall(r'\b\w+\b', content))
        factors['word_overlap'] = len(query_words & content_words) / max(len(query_words), 1)

        # 2. –í–∞–∂–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã (–¥–ª–∏–Ω–Ω—ã–µ —Å–ª–æ–≤–∞, —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ —Ç–µ—Ä–º–∏–Ω—ã)
        important_words = [w for w in query_words if len(w) > 4]
        important_overlap = sum(1 for term in important_words if term in content)
        factors['semantic_match'] = important_overlap / max(len(important_words), 1) if important_words else 0.5

        # 3. –ê–≤—Ç–æ—Ä–∏—Ç–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–±–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ—Å—Ç–æ–π –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π)
        factors['authority'] = 0.7  # –°—Ä–µ–¥–Ω–∏–π –≤–µ—Å –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π

        # 4. –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å (–±–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ)
        factors['recency'] = 0.5

        # 5. –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (–µ—Å–ª–∏ –ø—Ä–∏–º–µ–Ω–∏–º–æ)
        if user_context:
            # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
            pass

        return factors

    def _hybrid_search(self, query: str, k: int = 5, vector_weight: float = 0.7) -> List[Tuple[Document, float]]:
        """–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + BM25"""
        if self.vectorstore is None:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        
        # 1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        vector_results = self.vectorstore.similarity_search_with_score(query, k=k * 2)
        
        # 2. BM25 –ø–æ–∏—Å–∫ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
        if self.use_hybrid_search and self.bm25_index is not None and self.documents is not None:
            try:
                query_tokens = query.lower().split()
                bm25_scores = self.bm25_index.get_scores(query_tokens)
                
                # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ BM25 scores –ø–æ —Ç–µ–∫—Å—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞
                bm25_scores_dict = {}
                for idx, doc_score in enumerate(bm25_scores):
                    if idx < len(self.documents):
                        doc = self.documents[idx]
                        doc_key = (doc.page_content[:100] + 
                                  doc.metadata.get('source', '')[:50])
                        bm25_scores_dict[doc_key] = doc_score
                
                # –ö–æ–º–±–∏–Ω–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                combined = []
                for doc, distance in vector_results:
                    vector_score = 1 / (1 + distance)  # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –≤ –æ—Ü–µ–Ω–∫—É
                    
                    # –ù–∞—Ö–æ–¥–∏–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–π BM25 score
                    doc_key = (doc.page_content[:100] + 
                              doc.metadata.get('source', '')[:50])
                    bm25_score = bm25_scores_dict.get(doc_key, 0.0)
                    
                    # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ, –∏—â–µ–º –ø–æ –Ω–∞—á–∞–ª—É —Ç–µ–∫—Å—Ç–∞
                    if bm25_score == 0.0:
                        for key, score in bm25_scores_dict.items():
                            if doc.page_content[:50] in key or key[:50] in doc.page_content[:100]:
                                bm25_score = score
                                break
                    
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º BM25 score
                    if len(bm25_scores) > 0:
                        max_bm25 = float(max(bm25_scores))
                    else:
                        max_bm25 = 1.0
                    normalized_bm25 = (bm25_score / max_bm25) if max_bm25 > 0 else 0.0
                    
                    # Reciprocal Rank Fusion (RRF) - –∫–æ–º–±–∏–Ω–∞—Ü–∏—è —Ä–∞–Ω–≥–æ–≤
                    rrf_score = (vector_weight * vector_score) + ((1 - vector_weight) * normalized_bm25)
                    combined.append((doc, rrf_score))
                
                combined.sort(key=lambda x: x[1], reverse=True)
                return combined
            except Exception as e:
                print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–∏–±—Ä–∏–¥–Ω–æ–º –ø–æ–∏—Å–∫–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π: {e}")
                import traceback
                traceback.print_exc()
        
        # Fallback: —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
        results = []
        for doc, distance in vector_results:
            score = 1 / (1 + distance)
            results.append((doc, score))
        
        return results
    
    def get_relevant_context_with_scores(self, query: str, k: int = 3, score_threshold: float = 0.5, 
                                        user_context: dict = None):
        """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Å –æ—Ü–µ–Ω–∫–∞–º–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if self.vectorstore is None:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –∏–ª–∏ –æ–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π
        if self.use_hybrid_search and self.bm25_index is not None and self.documents is not None:
            results = self._hybrid_search(query, k=k * 2, vector_weight=0.7)
        else:
            # –û–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_results = self.vectorstore.similarity_search_with_score(query, k=k * 2)
            results = []
            for doc, distance in vector_results:
                score = 1 / (1 + distance)
                results.append((doc, score))
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–æ—Ä–æ–≥—É
        filtered_results = [(doc, score) for doc, score in results if score >= score_threshold]
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–ª—É—á—à–µ–Ω–Ω—ã–π re-ranking
        if len(filtered_results) > k:
            filtered_results = self._advanced_rerank_results(query, filtered_results, top_k=k, user_context=user_context)
        else:
            # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –º–µ–Ω—å—à–µ k, –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–∏–º–µ–Ω—è–µ–º re-ranking –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ—Ä—è–¥–∫–∞
            filtered_results = self._advanced_rerank_results(query, filtered_results, user_context=user_context)
        
        return filtered_results
    
    def get_relevant_context(self, query: str, k: int = 3) -> List[Document]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        results = self.get_relevant_context_with_scores(query, k=k)
        return [doc for doc, score in results]
    
    def get_relevant_context_as_text(self, query: str, k: int = 3, score_threshold: float = 0.5, 
                                     user_context: dict = None) -> tuple[str, bool]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –≤–∏–¥–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            score_threshold: –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∞—Ü–∏–∏
            
        Returns:
            –ö–æ—Ä—Ç–µ–∂ (—Ç–µ–∫—Å—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞, —Ñ–ª–∞–≥ —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏)
        """
        results = self.get_relevant_context_with_scores(query, k, score_threshold, user_context)
        
        if not results:
            return "", False
        
        context_parts = [
            "–ù–∏–∂–µ –ø—Ä–∏–≤–µ–¥–µ–Ω—ã –≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π. –ò—Å–ø–æ–ª—å–∑—É–π –∏—Ö –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é.\n"
        ]
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –¥–ª—è –ª—É—á—à–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–∏–∑–∞—Ü–∏–∏
        seen_docs = {}
        
        for i, (doc, score) in enumerate(results, 1):
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
            doc_name = doc.metadata.get('source', '–±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π')
            
            # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –æ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–π —Ñ–∞–π–ª–æ–≤
            if '.' in doc_name:
                doc_name = doc_name.rsplit('.', 1)[0]
            
            if score >= 0.8:
                relevance_label = "–≤—ã—Å–æ–∫–∞—è"
            elif score >= 0.6:
                relevance_label = "—Å—Ä–µ–¥–Ω—è—è"
            else:
                relevance_label = "–Ω–∏–∑–∫–∞—è"
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤—ã–≤–æ–¥ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            context_parts.append(
                f"---\n"
                f"–§—Ä–∞–≥–º–µ–Ω—Ç {i}\n"
                f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {relevance_label} ({score:.1%})\n"
                f"–¢–µ–∫—Å—Ç:\n{doc.page_content}\n"
            )
            
            # –°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏
            if doc_name not in seen_docs:
                seen_docs[doc_name] = score

        return "\n".join(context_parts), True

"""
RAG —Å–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π.
–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π, —Ä–∞–∑–±–∏–≤–∞–µ—Ç –Ω–∞ —á–∞–Ω–∫–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.
"""
import sys
import os
from pathlib import Path
from typing import List, Optional

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
try:
    from langchain_huggingface import HuggingFaceEmbeddings
except ImportError:
    # Fallback –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from tqdm import tqdm


class RAGService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π —á–µ—Ä–µ–∑ RAG"""
    
    def __init__(self, knowledge_base_path: str, vectorstore_dir: Optional[str] = None):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç RAG —Å–µ—Ä–≤–∏—Å
        
        Args:
            knowledge_base_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π
            vectorstore_dir: –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: vectorstore/)
        """
        self.knowledge_base_path = Path(knowledge_base_path)
        self.vectorstore_dir = Path(vectorstore_dir) if vectorstore_dir else Path(__file__).parent / "vectorstore"
        self.vectorstore_dir.mkdir(exist_ok=True)
        self.vectorstore = None
        self.embeddings = None
        self._initialize()
    
    def _get_index_path(self) -> Path:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–º—É –∏–Ω–¥–µ–∫—Å—É"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
        kb_name = self.knowledge_base_path.stem
        return self.vectorstore_dir / f"{kb_name}_faiss_index"
    
    def _should_rebuild_index(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        
        Returns:
            True –µ—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å, False –µ—Å–ª–∏ –º–æ–∂–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π
        """
        index_path = self._get_index_path()
        
        # –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –Ω—É–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å
        if not index_path.exists():
            return True
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∏–∑–º–µ–Ω–∏–ª—Å—è –ª–∏ —Ñ–∞–π–ª –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        kb_mtime = os.path.getmtime(self.knowledge_base_path)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Ä–µ–º—è –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–∞
        # –ò—â–µ–º —Ñ–∞–π–ª index.faiss (–æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª –∏–Ω–¥–µ–∫—Å–∞)
        index_file = index_path / "index.faiss"
        if not index_file.exists():
            return True
        
        index_mtime = os.path.getmtime(index_file)
        
        # –ï—Å–ª–∏ –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π –Ω–æ–≤–µ–µ –∏–Ω–¥–µ–∫—Å–∞, –Ω—É–∂–Ω–æ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å
        return kb_mtime > index_mtime
    
    def _initialize(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –±–∞–∑—É –∑–Ω–∞–Ω–∏–π –∏ —Å–æ–∑–¥–∞–µ—Ç/–∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
        print("\n" + "="*60)
        print("üìö –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø RAG –°–ï–†–í–ò–°–ê")
        print("="*60)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è embeddings (–Ω—É–∂–Ω–∞ –≤—Å–µ–≥–¥–∞)
        print("\n[1/4] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ embeddings...")
        print("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 –º–∏–Ω—É—Ç—ã –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)")
        print("   –ú–æ–¥–µ–ª—å: intfloat/multilingual-e5-base")
        print("   –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace...")
        sys.stdout.flush()
        
        try:
            self.embeddings = HuggingFaceEmbeddings(
                model_name="intfloat/multilingual-e5-base",
                model_kwargs={'device': 'cpu'}
            )
            print("   ‚úì –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        except Exception as e:
            print(f"   ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            raise
        
        index_path = self._get_index_path()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞–≤–∞—Ç—å –∏–Ω–¥–µ–∫—Å
        if not self._should_rebuild_index():
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –∏–Ω–¥–µ–∫—Å
            print(f"\n[2/4] –ó–∞–≥—Ä—É–∑–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
            print(f"   –ü—É—Ç—å: {index_path}")
            try:
                self.vectorstore = FAISS.load_local(
                    str(index_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print("   ‚úì –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
                print("\n" + "="*60)
                print("‚úÖ –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
                print("="*60 + "\n")
                return
            except Exception as e:
                print(f"   ‚ö† –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –∏–Ω–¥–µ–∫—Å–∞: {e}")
                print("   –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å...")
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å
        print("\n[2/4] –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∏–∑ —Ñ–∞–π–ª–∞...")
        loader = TextLoader(
            str(self.knowledge_base_path),
            encoding='utf-8'
        )
        documents = loader.load()
        print(f"‚úì –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω: {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç(–æ–≤)")
        
        # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏
        print("\n[3/4] –†–∞–∑–±–∏–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
        )
        chunks = text_splitter.split_documents(documents)
        print(f"‚úì –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Ä–∞–∑–±–∏—Ç–∞ –Ω–∞ {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
        print(f"\n[4/4] –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
        print("   (–≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å 1-3 –º–∏–Ω—É—Ç—ã, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...)")
        print("   –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏ –∏–Ω–¥–µ–∫—Å–∞ FAISS...")
        sys.stdout.flush()
        
        try:
            self.vectorstore = FAISS.from_documents(chunks, self.embeddings)
            print("   ‚úì –ò–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω —É—Å–ø–µ—à–Ω–æ!")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å –Ω–∞ –¥–∏—Å–∫
            print(f"\n[5/5] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –Ω–∞ –¥–∏—Å–∫...")
            print(f"   –ü—É—Ç—å: {index_path}")
            self.vectorstore.save_local(str(index_path))
            print("   ‚úì –ò–Ω–¥–µ–∫—Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω!")
        except Exception as e:
            print(f"\n   ‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω–¥–µ–∫—Å–∞: {e}")
            raise
        
        print("\n" + "="*60)
        print("‚úÖ –ë–ê–ó–ê –ó–ù–ê–ù–ò–ô –ì–û–¢–û–í–ê –ö –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Æ!")
        print("="*60 + "\n")
    
    def get_relevant_context(self, query: str, k: int = 3) -> List[Document]:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if self.vectorstore is None:
            raise ValueError("–í–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
        
        # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        docs = self.vectorstore.similarity_search(query, k=k)
        return docs
    
    def get_relevant_context_as_text(self, query: str, k: int = 3) -> str:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –≤–∏–¥–µ —Ç–µ–∫—Å—Ç–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            
        Returns:
            –¢–µ–∫—Å—Ç —Å —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–º–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞–º–∏
        """
        docs = self.get_relevant_context(query, k)
        context_parts = []
        for i, doc in enumerate(docs, 1):
            context_parts.append(f"[–§—Ä–∞–≥–º–µ–Ω—Ç {i}]\n{doc.page_content}\n")

        # –í—ã–≤–æ–¥–∏–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª—å, —á—Ç–æ–±—ã –≤–∏–¥–µ—Ç—å, —á—Ç–æ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ LLM
        print("\n--- RAG: –≤—ã–±—Ä–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã ---")
        for i, part in enumerate(context_parts, 1):
            print(f"[{i}] {part}")
        print("--- –ö–æ–Ω–µ—Ü —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ ---\n")

        return "\n".join(context_parts)

